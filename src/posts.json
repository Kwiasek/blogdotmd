{"posts": [
    {
        "id": 1,
        "title": "Extracting Histogram of Gradients with OpenCV",
        "date": "2023-11-11",
        "content": "Besides the feature descriptor generated by SIFT, SURF, and ORB, as in the [previous post](https://machinelearningmastery.com/opencv_sift_surf_orb_keypoints/), the Histogram of Oriented Gradients (HOG) is another feature descriptor you can obtain using OpenCV. HOG is a robust feature descriptor widely used in computer vision and image processing for object detection and recognition tasks. It captures the distribution of gradient orientations in an image and provides a powerful representation invariant to changes in illumination and shadowing.\n\nIn this post, you will learn about HOG. Specifically, you will know:\n\n- What is HOG, and how is it related to an image\n- How to compute it in OpenCV\n\nLet’s get started.\n\n![](https://machinelearningmastery.com/wp-content/uploads/2023/11/alexas_fotos-A0qtF_zwx_M-unsplash-scaled.jpg)\n\nExtracting Histogram of Gradients with OpenCV  \nPhoto by [Alexas_Fotos](https://unsplash.com/photos/white-and-black-piglet-on-gray-rocky-ground-during-daytime-A0qtF_zwx_M). Some rights reserved.\n\n## **Overview**\n\nThis post is divided into two parts; they are:\n\n- Understanding HOG\n- Computing HOG in OpenCV\n- Using HOg for People Detection\n\n## Understanding HOG\n\nThe concept behind the HOG algorithm is to compute the distribution of gradient orientations in localized portions of an image. HOG operates on a **window**, which is a region of fixed pixel size on the image. A window is divided into small spatial regions, known as a **block**, and a block is further divided into multiple **cells**. HOG calculates the gradient magnitude and orientation within each cell, and creates a histogram of gradient orientations. Then the histograms within the same block are concatenated.\n\nGradient measures how a pixel’s color intensity compares to its neighbors. The more drastic it changes, the higher the magnitude. The orientation tells which direction is the steepest gradient. Usually, this is applied on a single-channel image (i.e., grayscale), and each pixel can have its own gradient. HOG gathers all gradients from a block and puts them into a histogram.\n\nThe clever way of making a histogram in HOG is that the bins in a histogram are determined by the angle, but the value is interpolated between the closest bins. For example, if the bins are assigned values 0, 20, 40, and so on while the gradient was 10 at angle 30, a value of 5 was added to bins of 20 and 40. This way, HOG can effectively capture the texture and shape of objects within the image.\n\nHOG is particularly effective for detecting objects with distinguishable textures and patterns, making it a popular choice for tasks such as pedestrian detection and other forms of object recognition. With its ability to capture the distribution of gradient orientations, HOG provides a robust representation invariant to variations in lighting conditions and shadows.\n\n## Computing HOG in OpenCV\n\nOpenCV provides a straightforward method to compute the HOG descriptor, making it easily accessible for developers and researchers. Let’s take a look at a basic example of how to compute HOG in OpenCV:\n\n```\nimport cv2\n\n# Load the image and convert to grayscale\nimg = cv2.imread('image.jpg')\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# define each block as 4x4 cells of 64x64 pixels each\ncell_size = (128, 128)      # h x w in pixels\nblock_size = (4, 4)         # h x w in cells\nwin_size = (8, 6)           # h x w in cells\n\nnbins = 9  # number of orientation bins\nimg_size = img.shape[:2]  # h x w in pixels\n\n# create a HOG object\nhog = cv2.HOGDescriptor(\n    _winSize=(win_size[1] * cell_size[1],\n              win_size[0] * cell_size[0]),\n    _blockSize=(block_size[1] * cell_size[1],\n                block_size[0] * cell_size[0]),\n    _blockStride=(cell_size[1], cell_size[0]),\n    _cellSize=(cell_size[1], cell_size[0]),\n    _nbins=nbins\n)\nn_cells = (img_size[0] // cell_size[0], img_size[1] // cell_size[1])\n\n# find features as a 1xN vector, then reshape into spatial hierarchy\nhog_feats = hog.compute(img)\nhog_feats = hog_feats.reshape(\n    n_cells[1] - win_size[1] + 1,\n    n_cells[0] - win_size[0] + 1,\n    win_size[1] - block_size[1] + 1,\n    win_size[0] - block_size[0] + 1,\n    block_size[1],\n    block_size[0],\n    nbins)\nprint(hog_feats.shape)\n```\n\nHOG computes features for one window at a time. There are multiple blocks in a window. In a block, there are multiple “cells”. See the following illustration:\n\n![](https://machinelearningmastery.com/wp-content/uploads/2023/11/patch-and-cells.png)\n\nAssume this entire picture is one window. A window is divided into cells (green grids), and several cells are combined into one block (red and blue boxes). There are many overlapping blocks in one window, but all blocks are the same size.\n\nEach cell is of a fixed size. In the above, you used 64×64 pixels in a cell. Each block has an equal number of cells. In the above, you used 4×4 cells in a block. Also, there is equal number of cells in a window; you used 8×6 cells above. However, we are not dividing an image into blocks or windows when we compute HOG. But instead,\n\n1. Consider a window as a sliding window on the image, in which the sliding window’s stride size is the size of one cell, i.e., it slides across one cell at a time\n2. We divide the window into cells of fixed size\n3. We set up the second sliding window that matches the block size and scan the window. It slides across one cell at a time\n4. Within a block, HOG is computed from each cell\n\nThe returned HOG is a vector for the entire image. In the code above, you reshaped it to make it clear the hierarchy of windows, blocks, cells, and histogram bins. For example, `hog_feats[i][j]` corresponds to the window (in numpy slicing syntax):\n\n|   |   |\n|---|---|\n|1<br><br>2|img[n_cells[1]*i : n_cells[1]*i+(n_cells[1]*win_size[1]),<br><br>    n_cells[0]*j : n_cells[0]*j+(n_cells[0]*win_size[0])]|\n\nOr, equivalently, the window with the cell (i,j) at the top left corner.\n\nA sliding window is a common technique in object detection because you cannot be sure a particular object lies exactly in a grid cell. Making smaller cells but larger windows is a better way to catch the object than just seeing a part of it. However, there’s a limitation: An object larger than the window will be missed. Also, an object too small may be dwarfed by other elements in the window.\n\nUsually, you have some downstream tasks associated with HOG, such as running an SVM classifier on the HOG features for object detection. In this case, you may want to reshape the HOG output into vectors of the entire block rather than in the hierarchy of each cell like above.\n\n## Using HOG for People Detection\n\nThe feature extraction technique in the code above is useful if you want to get the raw feature vectors for other purposes. But for some common tasks, OpenCV comes with pre-trained machine learning models for your disposal without much effort.\n\nLet’s consider the photo from the following URL (save it as `people.jpg`):\n\n- [https://unsplash.com/photos/people-crossing-on-pedestrian-lane-near-buildings-during-daytime-JfBj_rVOhKY](https://unsplash.com/photos/people-crossing-on-pedestrian-lane-near-buildings-during-daytime-JfBj_rVOhKY)\n\n![](https://machinelearningmastery.com/wp-content/uploads/2023/11/people1-scaled.jpg)\n\nA photo is used as an example to detect people using HOG.  \nPhoto by [Chris Dickens](https://unsplash.com/photos/people-crossing-on-pedestrian-lane-near-buildings-during-daytime-JfBj_rVOhKY). Some rights reserved.\n\nThis is a picture of people crossing a street. OpenCV has a “people detector” in HOG that was trained on a 64×128 pixel window size. Using it to detect people in a photo is surprisingly simple:\n\n```\nimport cv2\n\n# Load the image and convert it to grayscale\nimg = cv2.imread('people.jpg')\n\nhog = cv2.HOGDescriptor()\nhog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n\n# Detect people in the image\nlocations, confidence = hog.detectMultiScale(img)\n\n# Draw rectangles around the detected people\nfor (x, y, w, h) in locations:\n    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 5)\n\n# Display the image with detected people\ncv2.imshow('People', img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n```\n\nIn the above, you created a HOG descriptor with the parameters from `cv2.HOGDescriptor_getDefaultPeopleDetector() will initialize` an SVM classifier to detect a particular object, which in this case is people.\n\nYou call the descriptor on an image and run the SVM in one pipeline using `hog.detectMultiScale(img)`, which returns the **bounding boxes** for each object detected. While the window size is fixed, this detection function will resize the image in multiple scales to find the best detection result. Even so, the bounding boxes returned are not tight. The code above also annotates the people detected by marking the bounding box on the image. You may further filter the result using the confidence score reported by the detector. Some filtering algorithms, such as non-maximum suppression, may be appropriate but are not discussed here. The following is the output:\n\n![](https://machinelearningmastery.com/wp-content/uploads/2023/11/people-hog-scaled.jpg)\n\nBounding box as produced by the people detector using HOG in OpenCV\n\nYou can see such detectors can find people only if the full body is visible. The output has false positives (non-people detected) and false negatives (people not detected). Using it to count all people in a crowd scene would be challenging. But it is a good start to see how easily you can get something done using OpenCV.\n\nUnfortunately, there are not any detectors that come with OpenCV other than people. But you can train your own SVM or other models using the HOG as feature vectors. Facilitating a machine learning model is the key point of extracting feature vectors from an image.\n\n## **Further Reading**\n\nThis section provides more resources on the topic if you are looking to go deeper.\n\n### **Books**\n\n- [Mastering OpenCV 4 with Python](https://www.amazon.com/Mastering-OpenCV-Python-practical-processing/dp/1789344913), 2019.\n\n### **Websites**\n\n- OpenCV, [https://opencv.org/](https://opencv.org/)\n- StackOverflow: OpenCV HOG Features Explanation: [https://stackoverflow.com/questions/44972099/opencv-hog-features-explanation](https://stackoverflow.com/questions/44972099/opencv-hog-features-explanation)\n\n## **Summary**\n\nIn this tutorial, you learned how to use HOG in OpenCV to extract feature vectors based on a sliding window. It is an effective approach to finding features that can help object detection.\n\nSpecifically, you learned:\n\n- How to fetch HOG features from an image\n- How to use the built-in HOG people detector from OpenCV"
    },
    {
        "id": 2,
        "title": "Image Feature Extraction in OpenCV: Keypoints and Description Vectors",
        "date": "2023-11-7",
        "content": "In the [previous post](https://machinelearningmastery.com/opencv_edges_and_corners/), you learned some basic feature extraction algorithms in OpenCV. The features are extracted in the form of classifying pixels. These indeed abstract the features from images because you do not need to consider the different color channels of each pixel, but to consider a single value. In this post, you will learn some other feature extract algorithms that can tell you about the image more concisely.\n\nAfter completing this tutorial, you will know:\n\n- What are keypoints in an image\n- What are the common algorithms available in OpenCV for extracting keypoints\n\nLet’s get started.\n\n ![](https://machinelearningmastery.com/wp-content/uploads/2023/11/silas-kohler-C1P4wHhQbjM-unsplash.jpg)\n\n Image Feature Extraction in OpenCV: Keypoints and Description Vectors  \nPhoto by [Silas Köhler](https://unsplash.com/photos/black-skeleton-keys-C1P4wHhQbjM), some rights reserved.\n\n## **Overview**\n\nThis post is divided into two parts; they are:\n\n- Keypoint Detection with SIFT and SURF in OpenCV\n- Keypoint Detection using ORB in OpenCV\n\n## **Prerequisites**\n\nFor this tutorial, we assume that you are already familiar with:\n\n- [Reading and displaying images using OpenCV](https://machinelearningmastery.com/?p=14402&preview=true)\n\n## Keypoint Detection with SIFT and SURF in OpenCV\n\nScale-Invariant Feature Transform (SIFT) and Speeded-Up Robust Features (SURF) are powerful algorithms for detecting and describing local features in images. They are named scale-invariant and robust because, compared to Harris Corner Detection, for example, its result is expectable even after some change to the image.\n\nThe SIFT algorithm applies Gaussian blur to the image and computes the difference in multiple scales. Intuitively, such a difference will be zero if your entire image is a single flat color. Hence this algorithm is called keypoint detection, which identifies a place in the image with the most significant change in pixel values, such as corners.\n\nThe SIFT algorithm derives certain “orientation” values for each keypoint and outputs a vector that represents the histogram of the orientation values.\n\nIt is found quite slow to run SIFT algorithm. Hence there is a speed-up version, SURF. It would be lengthy to describe the SIFT and SURF algorithms in detail but luckily, you do not need to understand too much to use it with OpenCV.\n\nLet’s look at an example using the following image:\n\n- [https://unsplash.com/photos/VSLPOL9PwB8](https://unsplash.com/photos/VSLPOL9PwB8)\n\nSimilar to the previous post, SIFT and SURF algorithms assume a grayscale image. This time, you need to create a detector first and apply to the image:\n\n```\nimport cv2\n\n# Load the image and convery to grayscale\n\nimg = cv2.imread('image.jpg')\n\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Initialize SIFT and SURF detectors\n\nsift = cv2.SIFT_create()\n\nsurf = cv2.xfeatures2d.SURF_create()\n\n# Detect key points and compute descriptors\n\nkeypoints_sift, descriptors_sift = sift.detectAndCompute(img, None)\n\nkeypoints_surf, descriptors_surf = surf.detectAndCompute(img, None)\n```\n\n**NOTE:** You may find difficulties to run the above code in your OpenCV installation. You may need to compile your own OpenCV module from scratch to make this run. It is because SIFT and SURF were patented so OpenCV considered them “non-free”. Since the SIFT patent is already expired (SURF is still in effect), you may find SIFT works fine if you downloaded a newer version of OpenCV.\n\nThe output of the SIFT or SURF algorithm are a list of keypoints and a numpy array of descriptors. The descriptors array is Nx128 for N keypoints, each represented by a vector of length 128. Each keypoint is an object with several attributes, such as the orientation angle.\n\nThere can be a lot of keypoints detected by default, because this helps one of the best use for detected keypoints — to find associations between distorted images.\n\nTo reduce the number of detected keypoint in the output, you can set a higher “contrast threshold” and lower “edge threshold” (default to be 0.03 and 10 respectively) in SIFT or increase the “Hessian threshold” (default 100) in SURF. These can be adjusted at the detector object using `sift.setContrastThreshold(0.03)`, `sift.setEdgeThreshold(10)`, and `surf.setHessianThreshold(100)` respectively.\n\nTo draw the keypoints on the image, you can use the `cv2.drawKeypoints()` function and apply the list of all keypoints to it. The complete code, using only the SIFT algorithm and set a very high threshold to keep only a few keypoints, is as follows:\n\n```\nimport cv2\n\n# Load the image and convery to grayscale\nimg = cv2.imread('image.jpg')\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Initialize SIFT detector\nsift = cv2.SIFT_create()\nsift.setContrastThreshold(0.25)\nsift.setEdgeThreshold(5)\n\n# Detect key points and compute descriptors\nkeypoints, descriptors = sift.detectAndCompute(img, None)\nfor x in keypoints:\n    print(\"({:.2f},{:.2f}) = size {:.2f} angle {:.2f}\".format(x.pt[0], x.pt[1], x.size, x.angle))\n\nimg_kp = cv2.drawKeypoints(img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\ncv2.imshow(\"Keypoints\", img_kp)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n```\n\nThe image created is as follows:\n\n![](https://machinelearningmastery.com/wp-content/uploads/2023/11/sift.jpg)\n\nKeypoints detected by the SIFT algorithm (zoomed in)  \nOriginal photo by [Gleren Meneghin](https://unsplash.com/photos/VSLPOL9PwB8), some rights reserved.\n\nThe function `cv2.drawKeypoints()` will not modify your original image, but to return a new one. In the picture above, you can see the keypoints drawn as circle proportional to its “size” with a stroke indicating the orientation. There are keypoints on the number “17” on the door as well as on the mail slots. But there are indeed more. From the for loop above, you can see that some keypoints are overlapped because there are multiple orientation angles found.\n\nIn showing the keypoints on image, you used the keypoint objects returned. However, you may find the feature vectors stored in `descriptors` useful if you want to further process the keypoints, such as running a clustering algorithm among them. But note that, you still need the list of keypoints for information such as the coordinates to match with the feature vectors.\n\n## Keypoint Detection using ORB in OpenCV\n\nSince the SIFT and SURF algorithms are patented, there is an incentive to develop a free alternative that doesn’t need to be licensed. It is a product of the OpenCV developers themselves.\n\nORB stands for Oriented FAST and Rotated BRIEF. It is a combination of two other algorithms, FAST and BRIEF with modifications to match the performance of SIFT and SURF. You do not need to understand the algorithm details to use it, and its output is also a list of keypoint objects, as follows:\n\n```\nimport cv2\n\n# Load the image and convery to grayscale\nimg = cv2.imread('image.jpg')\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Initialize SIFT detector\nsift = cv2.SIFT_create()\nsift.setContrastThreshold(0.25)\nsift.setEdgeThreshold(5)\n\n# Detect key points and compute descriptors\nkeypoints, descriptors = sift.detectAndCompute(img, None)\nfor x in keypoints:\n    print(\"({:.2f},{:.2f}) = size {:.2f} angle {:.2f}\".format(x.pt[0], x.pt[1], x.size, x.angle))\n\nimg_kp = cv2.drawKeypoints(img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\ncv2.imshow(\"Keypoints\", img_kp)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n```\n\nIn the above, you set the ORB to generate top 30 keypoints when you created the detector. By default, this number will be 500.\n\nThe detector returns a list of keypoints and a numpy array of descriptors (feature vector of each keypoint) exactly as before. However, the descriptors of each keypoint is now of length-32 instead of 128.\n\nThe generated keypoints is as follows:\n\n![](https://machinelearningmastery.com/wp-content/uploads/2023/11/orb.jpg)\n\nKeypoints detected by ORB algorithm  \nOriginal photo by [Gleren Meneghin](https://unsplash.com/photos/VSLPOL9PwB8), some rights reserved.\n\nYou can see, keypoints are generated roughly at the same location. The results are not exactly the same because there are overlapping keypoints (or offset by a very small distance) and easily the ORB algorithm reached the maximum count of 30. Moreover, the size are not comparable between different algorithms.\n\n## **Further Reading**\n\nThis section provides more resources on the topic if you are looking to go deeper.\n\n### **Books**\n\n- [Mastering OpenCV 4 with Python](https://www.amazon.com/Mastering-OpenCV-Python-practical-processing/dp/1789344913), 2019.\n\n### **Websites**\n\n- OpenCV, [https://opencv.org/](https://opencv.org/)\n- OpenCV Feature Detection and Description, [https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html](https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html)\n\n## **Summary**\n\nIn this tutorial, you learned how to apply OpenCV’s keypoint detection algorithms, SIFT, SURF, and ORB.\n\nSpecifically, you learned:\n\n- What is a keypoint in an image\n- How to find the keypoints and the associated description vectors using OpenCV functions.\n\nIf you have any questions, please leave a comment below."
    },
    {
        "id": 3,
        "date": "2023-11-7",
        "title": "Image Feature Extraction in OpenCV: Edges and Corners",
        "content": "In the world of computer vision and image processing, the ability to extract meaningful features from images is important. These features serve as vital inputs for various downstream tasks, such as object detection and classification. There are multiple ways to find these features. The naive way is to count the pixels. But in OpenCV, there are many routines to help you extract features from an image. In this post, you will see how OpenCV can help find some high-level features.\n\nAfter completing this tutorial, you will know:\n\n- Corner and edges can be extracted from an image\n- What are the common algorithms available in OpenCV for extracting corners and edges\n\nLet’s get started.\n\n![](https://machinelearningmastery.com/wp-content/uploads/2023/11/michael-barth-7Yp3v4Ol1jI-unsplash.jpg)\n\nImage Feature Extraction in OpenCV: Edges and Corners  \nPhoto by [Michael Barth](https://unsplash.com/photos/gray-building-under-calm-sky-7Yp3v4Ol1jI), some rights reserved.\n\n## **Overview**\n\nThis post is divided into three parts; they are:\n\n- Understanding Image Feature Extraction\n- Canny Edge Detection in OpenCV\n- Harris Corner Detection in OpenCV\n\n## **Prerequisites**\n\nFor this tutorial, we assume that you are already familiar with:\n\n- [Reading and displaying images using OpenCV](https://machinelearningmastery.com/?p=14402&preview=true)\n\n## Understanding Image Feature Extraction\n\nImage feature extraction involves identifying and representing distinctive structures within an image. Reading the pixels of an image is certainly one. But this is a low-level feature. A high-level feature of an image can be anything from edges, corners, or even more complex textures and shapes.\n\nFeatures are characteristics of an image. With these unique characteristics, you may be able to distinguish one image from another. This is the first step in computer vision. By extracting these features, you can create representations that are more compact and meaningful than merely the pixels of the image. It helps further analysis and processing.\n\nIn the following, you will learn the two basic but very common feature extraction algorithms. Both of them return a pixel-based classification in the format of numpy arrays.\n\n## Canny Edge Detection in OpenCV\n\nOver the years, there have been many algorithms developed for image feature extraction. They are not machine learning models, but closer to deterministic algorithms. These algorithms each aimed at a particular feature.\n\nOpenCV provides a rich set of tools and functions for image feature extraction. Let’s start with the first, Canny edge detection.\n\nFinding lines in an image is probably the simplest feature extraction. Its goal is to identify which pixel is on an edge. An edge is defined as a gradient on the pixel intensity. In other words, if there is an abrupt color change, it is considered an edge. But there are more details to it, so noises are excluded.\n\nLet’s consider the following image and save it as `image.jpg` in the local directory:\n\n- [https://unsplash.com/photos/VSLPOL9PwB8](https://unsplash.com/photos/VSLPOL9PwB8)\n\nAn example of finding and illustrating edges is as follows:\n\n```\nimport cv2\nimport numpy as np\n\n# Load the image\nimg = cv2.imread('image.jpg')\n\n# Convert to grayscale\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Detect edges using Canny method\nedges = cv2.Canny(gray, 150, 300)\n\n# Display the image with corners\nimg[edges == 255] = (255,0,0)\ncv2.imshow('Canny Edges', img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n```\n\nIn the above, the image is converted into grayscale and then called `cv2.Canny()` function. Grayscale images are required in many feature extraction algorithm because many are designed to work on a single color channel.\n\nThe argument to the `cv2.Canny()` function takes two numerical values, for minimum and maximum thresholds respectively. They are used in the **hysteresis thresholding** to consolidate pixels into edges. The higher the maximum, only the stronger edges are kept in the result. The higher the minimum, however, you will see more “disconnected edges” returned.\n\nThis function returns an numpy array that matched the pixel dimension of the image, which the value is either 0 (not on an edge) or 255 (on an edge). The code above color those pixels in blue. The result is as follows:\n\n![](https://machinelearningmastery.com/wp-content/uploads/2023/11/canny.jpg)\n\nResult of Canny edge detection  \nOriginal photo by [Gleren Meneghin](https://unsplash.com/photos/VSLPOL9PwB8), some rights reserved.\n\nYou should see the blue lines above marked the door and window and also outlined each brick. You adjust the two thresholds to see a different result.\n\n## Harris Corner Detection in OpenCV\n\nHarris Corner Detection is a method used to identify significant variations in intensity, which often correspond to the corners of objects in an image. OpenCV offers a simple and efficient implementation of this technique, allowing us to detect corners that serve as prominent features for image analysis and matching.\n\nExtracting corners from an image can be done in three steps:\n\n1. Convert the image into grayscale, because Harris corner detection algorithm works only on a single color channel\n2. Run `cv2.cornerHarris(image, blockSize, ksize, k)` and get a score for every pixel\n3. Identify which pixel is at the corner by comparing the score against the image maximum\n\nThe argument to `cornerHarris()` function include the neighborhood size `blockSize` and a kernel size `ksize`. Both are small positive integers but the latter must be an odd number. The final argument `k` is a positive floating point value that controls the sensitivity of corner detection. Too large such a value will make the algorithm mistake a corner as an edge. You may need to experiment with its value.\n\nAn example code, running Harris corner detection on the same image above:\n\n```\nimport cv2\nimport numpy as np\n\n# Load the image\nimg = cv2.imread('image.jpg')\n\n# Convert to grayscale\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Detect corners using the Harris method\ndst = cv2.cornerHarris(gray, 3, 5, 0.1)\n\n# Create a boolean bitmap of corner positions\ncorners = dst > 0.05 * dst.max()\n\n# Find the coordinates from the boolean bitmap\ncoord = np.argwhere(corners)\n\n# Draw circles on the coordinates to mark the corners\nfor y, x in coord:\n    cv2.circle(img, (x,y), 3, (0,0,255), -1)\n\n# Display the image with corners\ncv2.imshow('Harris Corners', img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n```\n\nThe image produced will be as follows:\n\n![](https://machinelearningmastery.com/wp-content/uploads/2023/11/harris.jpg)\n\nResult of Harris corner detection  \nOriginal photo by [Gleren Meneghin](https://unsplash.com/photos/VSLPOL9PwB8), some rights reserved.\n\nThe red dots were drawn by the `cv2.circle()` function inside the for loop above. They are just for illustration. The key idea is that the algorithm gives a score of each pixel of the image to tell how much it is believed to be a corner, or on an edge, or “flat” (i.e., neither). You need to control the sensitivity of your conclusion by comparing the score to the maximum among the entire image, in the line\n\n```\ncorners = dst &gt; 0.05 * dst.max()\n```\n\nThe result is a Boolean numpy array `corners`, which is then converted into an array of coordinates using the `np.argwhere()` function.\n\nFrom the image above, you can see that Harris corner detection is not perfect, but if the corner is obvious enough, it can be detected.\n\n## **Further Reading**\n\nThis section provides more resources on the topic if you are looking to go deeper.\n\n### **Books**\n\n- [Mastering OpenCV 4 with Python](https://www.amazon.com/Mastering-OpenCV-Python-practical-processing/dp/1789344913), 2019.\n\n### **Websites**\n\n- OpenCV, [https://opencv.org/](https://opencv.org/)\n- OpenCV Feature Detection and Description, [https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html](https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html)\n- OpenCV Canny Edge Detection, [https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html](https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html)\n\n## **Summary**\n\nIn this tutorial, you learned how to apply OpenCV’s Canny Edge Detection and Harris Corner Detection algorithms on an image\n\nSpecifically, you learned:\n\n- These are pixel-based algorithms that classify each pixel into edge or non-edge, or corner or non-corner\n- How to apply these algorithms using OpenCV functions to an image and interpret the result"
    }
]
}